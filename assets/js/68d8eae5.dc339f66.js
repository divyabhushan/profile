"use strict";(self.webpackChunkmy_portfolio=self.webpackChunkmy_portfolio||[]).push([[317],{8495:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"/2019/06/10/devops-interview-questions-answers","metadata":{"permalink":"/profile/blog/2019/06/10/devops-interview-questions-answers","source":"@site/blog/2019-06-10-devops-interview-questions-answers.md","title":"DevOps Interview Questions and Answers - Part I - Basics","description":"DevOps is a set of practices that combines software development and IT operations. Here is a list of the top interview questions asked to bag a role in DevOps field.","date":"2019-06-10T00:00:00.000Z","formattedDate":"June 10, 2019","tags":[],"readingTime":13.595,"hasTruncateMarker":true,"authors":[{"name":"Divya Bhushan","url":"https://github.com/divyabhushan","imageURL":"https://avatars0.githubusercontent.com/u/11659160?"}],"frontMatter":{"title":"DevOps Interview Questions and Answers - Part I - Basics","author":"Divya Bhushan","authorURL":"https://github.com/divyabhushan","authorImageURL":"https://avatars0.githubusercontent.com/u/11659160?"},"nextItem":{"title":"Docker Vs Virtual Machines(VMs)","permalink":"/profile/blog/2019/05/10/docker-vs-VMs"}},"content":"DevOps is a set of practices that combines software development and IT operations. Here is a list of the top interview questions asked to bag a role in DevOps field.\\n\\n\x3c!--truncate--\x3e\\n\\n1. [What is Devops?](#1-what-is-devops)\\n2. [What are the tools you have used in devops?](#2-what-are-some-of-the-tools-you-have-used-in-devops-approach)\\n3. [What is Git?](#3-what-is-git)\\n4. [What is a git commit object? How is it read?](#4-what-is-a-git-commit-object-how-is-it-read)\\n5. [What is the difference between a `git reset` and `git revert`](#5-what-is-the-difference-between-a-git-reset-and-a-git-revert)\\n6. [](#6-how-does-git-rebase-work-when-should-you-rebase-your-work-instead-of-a-git-merge)\\n\\n\\n## 1. What is DevOps?\\n\\nDevOps is an approach to collaborate the development and operations teams for a better, bug-free continuous delivery and integration of the source code.  \\nDevOps is about automating the entire SDLC(Software Development Life Cycle) process with the implementation of CI/CD practices.\\nCI/CD are the Continuous integration and continuous deployment methodologies.  \\nEvery source code check-in will automatically build and unit test the entire code against a  \\nproduction like environment and continuously deployed to production environment after it passes its automated tests.  \\nThat eliminates the long feedback, bug-fix, and product enhancements loops between every  \\nRelease.\\nEvery team takes the accountability of the entire product right from the requirement analysis to documentation to coding, testing in development environments, code deployment and continuous improvements in terms of bugs and feedback from reviewers and the customers.\\n\\n## 2. What are some of the tools you have used in DevOps approach?]\\n\\nConsidering DevOps to be an ideology towards achieving a quality product, every organization has its own guidelines and approach towards it.  \\nSome of the popular tools I have used are:\\n\\n* Git as a Distributed VCS to manage the source code.\\n* Jenkins to achieve CI/CD (Continuous Integration and Continuous Delivery) +plugins\\n* Puppet for Configuration Management and Deployment tool\\n* Nagios for Continuous Monitoring; and\\n* Docker for containerization.\\n\\n## 3. What is Git?\\n\\nGit is a Distributed Version Control System; used to logically store and backup the entire history of how your project source code has developed, keeping a track of every version change of the code.\\n\\nGit facilitates very flexible and efficient branching and merging of your code with other collaborators.Being distributed git is extremely fast and more reliable as every developer has his own local copy of the entire repository.\\n\\nGit allows you to undo the mistakes in the source code at different tiers of its architecture namely- Working directory, Staging (Index) area, Local repository, and Remote repository.\\n\\nUsing Git we can always get an older version of our source code and work on it.Git tracks every bit of data as it checksums every file into unique hash codes referring to them via pointers.\\n\\nTo summarize Git is the most efficient and widely used VCS, used by major companies like Linux, Google, Facebook, Microsoft, Twitter, LinkedIn, Netflix, Android, Amazon, IBM, Apple IOS to name a few\u2026\\n\\n## 4. What is a git commit object? How is it read?\\n\\nWhen a project repository is initialized to be a git repository, git stores all its metadata in a hidden folder \u201c.git\u201d under the project root directory.  \\nGit repository is a collection of _objects._ \\n\\nGit has 4 types of objects \u2013 blobs, trees, tags, and commits.\\n\\nEvery commit creates a new _commit object_ with a unique SHA-1 hash_id.  \\nEach commit object has a pointer reference to the tree object, its parent object, author, committer and the commit message.  \\n![DevOps](https://d2o2utebsixu4k.cloudfront.net/media/images/1552023479018-DevOps--set2-.jpg)\\n\\nDiagram: Single Commit object\\n\\nTo see the commit log message along with the textual diff of the code, run:  \\n```bash\\ngit show <commit_id>\\n```\\n\\n```bash\\nDivya1@Divya:initialRepo [master] $git show f9354cb\\ncommit f9354cb08d91e80cabafd5b54d466b6055eb2927\\nAuthor: divya bhushan <divya_bhushan@hotmail.com>\\nDate:   Mon Feb 11 23:39:24 2019 +0100\\n    Add database logs.\\ndiff --git a/logs/db.log b/logs/db.log\\nnew file mode 100644\\nindex 0000000..f8854b0\\n--- /dev/null\\n+++ b/logs/db.log\\n@@ -0,0 +1 @@\\n+database logs\\n```\\n\\nTo read a commit object git has \'git cat-file\u2019 utility.  \\n\\n```bash\\nDivya1@Divya:initialRepo [master] $git cat-file -p f9354cb\\ntree 2a85825b8d20918350cc316513edd9cc289f8349\\nparent 30760c59d661e129329acfba7e20c899d0d7d199\\nauthor divya bhushan <divya_bhushan@hotmail.com> 1549924764 +0100\\ncommitter divya bhushan <divya_bhushan@hotmail.com> 1549924764 +0100 \\nAdd database logs.\\n```\\n\\nA tree object is like an OS directory that stores references to other directories and files (blob type).  \\n\\n```bash\\nDivya1@Divya:initialRepo [master] $git cat-file -p 2a85825b8d20918350cc316513edd9cc289f8349\\n100755 blob 054acd444517ad5a0c1e46d8eff925e061edf46c README.md\\n040000 tree dfe42cbaf87e6a56b51dab97fc51ecedfc969f39 code\\n100644 blob e08d4579f39808f3e2830b5da8ac155f87c0621c dockerfile\\n040000 tree 014e65a65532dc16a6d50e0d153c222a12df4742   logs\\n```\\n\\n## 5. What is the difference between a git reset and a git revert.\\n\\n* git revert is used to record some new commits to reverse the effect of some earlier commits/snapshot of a project.\\n* Instead of removing the commit from the project history, it figures out how to undo the changes introduced by the commit & appends a new commit with the resulting content in the current branch.\\n\\n![difference-between-a-git-reset-and-a-git](https://d2o2utebsixu4k.cloudfront.net/media/images/1552024197659-What-is-the-difference-between-a-git-reset-and-a-git-revert.jpg)\\n\\n*   **Usage:** git revert <commit_id>\\n*   **Use:** To undo an entire commit from your project history; removing a bug introduced by a commit.\\n\\n**Reset Vs Revert**\\n\\n*   git \u201creset\u201d: resets the project to a previous snapshot erasing the changes.\\n*   git \u201crevert\u201d does not change the project history unlike git \u201creset\u201d\\n*   Git \u201crevert\u201d undoes the commit id changes and applies the undo work as a new commit id object.\\n\\n## 6. How does \'git rebase\u2019 work? When should you rebase your work instead of a \'git merge\u2019?\\n\\nThere are scenarios wherein one would like to merge a quickfix or feature branch with not a huge commit history into another \'dev\u2019 or \'uat\u2019 branch and yet maintain a linear history.\\n\\nA non-fast forward \'git merge\u2019 would result in a diverged history. Also when one wants the feature merged commits to be the latest commits; \'git rebase\u2019 is an appropriate way of merging the two branches.\\n\\n\'git rebase\u2019 replays the commits on the current branch and place them over the tip of the rebased branch.Since it replays the commit ids, rebase rewrites commit objects and create a new object id(SHA-1). Word of caution: Do not use it if the history is on release/production branch and being shared on the central server. Limit the rebase on your local repository only to rebase quickfix or feature branches.\\n\\n**Steps:**\\n\\nSay there is a \'dev\u2019 branch that needs a quick feature to be added along with the test cases from \'uat\u2019 branch.\\n\\n* **Step 1:** Branch out \'new-feature\u2019 branch from \'dev\u2019.\\n\\nDevelop the new feature and make commits in \'new-feature\u2019 branch.\\n\\n```bash\\n[dev ] $git checkout -b new-feature\\n[new-feature ] $ git add lib/commonLibrary.sh && git commit -m \u201cAdd commonLibrary file\u201d\\nDivya1@Divya:rebase_project [new-feature] $git add lib/commonLibrary.sh && git commit -m \'Add commonLibrary file\'Divya1@Divya:rebase_project [new-feature] $git add feature1.txt && git commit -m \'Add feature1.txt\'\\nDivya1@Divya:rebase_project [new-feature] $git add feature2.txt && git commit -m \'Add feature2.txt\'\\n```\\n\\n* **Step 2:** Merge \'uat\u2019 branch into \'dev\u2019\\n\\n```bash\\n[dev] $ git merge uat\\n```\\n\\n* **Step 3:** Rebase \'new-feature\u2019 on \'dev\u2019\\n\\n```bash\\nDivya1@Divya:rebase_project [dev] $git checkout new-featureDivya1@Divya:rebase_project [new-feature] $git rebase dev\\nFirst, rewinding head to replay your work on top of it...\\nApplying: Add commonLibrary file\\nApplying: Add feature1.txt\\nApplying: Add feature2.txt\\n```\\n\\n* **Step 4:** Switch to \'dev\u2019 branch and merge \'new-feature\u2019 branch, this is going to be a fast-forward merge as \'new-feature\u2019 has already incorporated \'dev\u2019+\u2019uat\u2019 commits.\\n\\n```bash\\nDivya1@Divya:rebase_project [new-feature] $git checkout dev\\nDivya1@Divya:rebase_project [dev] $git merge new-feature\\nUpdating 5044e24..3378815\\nFast-forward\\nfeature1.txt         | 1 +\\nfeature2.txt         | 1 +\\nlib/commonLibrary.sh | 16 ++++++++++++++++\\n3 files changed, 18 insertions(+)\\ncreate mode 100644 feature1.txt\\ncreate mode 100644 feature2.txt\\ncreate mode 100644 lib/commonLibrary.sh\\n```\\n\\nthis will result in linear history with \'new-feature\u2019 results being at the top and \'dev\u2019 commits being pushed later.\\n\\n* **Step 5:** View the history of \'dev\u2019 after merging \'uat\u2019 and \'new-feature\u2019 (rebase)\\n\\n```bash\\nDivya1@Divya:rebase_project [dev] $git hist\\n* 3378815 2019-02-14 | Add feature2.txt (HEAD -> dev, new-feature) [divya bhushan]\\n* d3859c5 2019-02-14 | Add feature1.txt [divya bhushan]\\n* 93b76f7 2019-02-14 | Add commonLibrary file [divya bhushan]\\n*   5044e24 2019-02-14 | Merge branch \'uat\' into dev [divya bhushan]\\n|  \\n| * bb13fb0 2019-02-14 | End of uat work. (uat) [divya bhushan]\\n| * 0ab2061 2019-02-14 | Start of uat work. [divya bhushan]\\n* | a96deb1 2019-02-14 | End of dev work. [divya bhushan]\\n* | 817544e 2019-02-14 | Start of dev work. [divya bhushan]\\n|/  \\n* 01ad76b 2019-02-14 | Initial project structure. (tag: v1.0, master) [divya bhushan]\\n```\\n\\n:::note\\nNOTE: \'dev\u2019 will show a diverged commit history for \'uat\u2019 merge and a linear history for \'new-feature\u2019 merge.\\n:::\\n\\n## 7. What is a Docker? Explain its role in DevOps.\\n\\nEvery source code deployment needs to be portable and compatible on every device and environment.\\n\\nApplications and their run time environment such as libraries and other dependencies like binaries, jar files, configuration files etc.. are bundled up(packaged) in a **Container**.\\n\\n_Containers_ as a whole are portable, consistent and compatible with any environment.\\n\\nIn development words, a developer can run its application in any environment: dev, uat, preprod and production without worrying about the run-time dependencies of the application.\\n\\n*   Docker is a container platform.\\n*   Docker is a framework that provides an abstraction layer to manage containers.\\n*   _Docker_ is a containerization engine, which automates packaging, shipping, and deployment of any software applications or _Containers._\\n*   Docker also lets us test the code and then deploy it in production.\\n*   Docker along with Jenkins (a Continuous Integration tool) and Git plugin contributes in CI/CD process.\\n\\n## 8. What is a Docker image? How are they shared and accessed?\\n\\nA developer writes code instructions to define all the applications and its dependencies in a file called a \u201cDockerfile\u201d.Dockerfile is used to create a \'Docker image\' using the `docker build <directory>` command.The build command is run by the docker daemon.  \\n  \\nWhen you run a Docker image \u201cContainers\u201d are created. Containers are runtime instances of a Docker image.\\n\\n*   A Container can have many images.\\n*   Docker containers are stored in a docker registry on docker host.\\n*   Docker has a client-Server architecture.\\n*   Docker registry is generally pushed and shared on a Docker hub (Remote server).\\n\\n![Docker-image-How-are-they-shared-and-accessed](https://d2o2utebsixu4k.cloudfront.net/media/images/1552025560274-What-is-a-Docker-image-How-are-they-shared-and-accessed.jpg)\\n\\nImage credit: [docs.docker.com](https://docs.docker.com/engine/docker-overview/)\\n\\n*   Other developers \'Docker pull\u2019 these registry images and create containers in their own environment.\\n*   Developers can run their applications in the same docker container as their peers.\\n*   This way you can get the same test environment on different servers with the same applications and dependencies.\\n\\n## 9. How do you work on a container image?\\n\\n```bash\\n--Get docker images from docker hub or your docker repository\\n\\ndocker pull busybox\\ndocker pull centos\\ndocker pull divyabhushan/myrepo\\nDivya1@Divya:~ $docker pull divyabhushan/myrepo\\nUsing default tag: latest\\nlatest: Pulling from divyabhushan/myrepo\\n6cf436f81810: Pull complete\\n987088a85b96: Pull complete\\nb4624b3efe06: Pull complete\\nd42beb8ded59: Pull complete\\nd08b19d33455: Pull complete\\n80d9a1d33f81: Pull complete\\nDigest: sha256:c82b4b701af5301cc5d698d963eeed46739e67aff69fd1a5f4ef0aecc4bf7bbf\\nStatus: Downloaded newer image for divyabhushan/myrepo:latest\\n```\\n\\n```bash\\n--List the docker images\\n\\nDivya1@Divya:~ $docker images\\nREPOSITORY            TAG IMAGE ID            CREATED SIZE\\ndivyabhushan/myrepo   latest 72a21c221add        About an hour ago 88.1MB\\nbusybox               latest 3a093384ac30        5 weeks ago 1.2MB\\ncentos                latest 1e1148e4cc2c        2 months ago 202MB\\n```\\n\\n```bash\\n--Create a docker container by running the docker image\\n\\n--pass a shell argument  : uname -a\\n\\nDivya1@Divya:~ $docker run centos uname -a\\nLinux c70fc2da749a 4.9.125-linuxkit #1 SMP Fri Sep 7 08:20:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\\n```\\n\\n--Docker images can be built by reading a dockerfile  \\n![dockerfile](https://d2o2utebsixu4k.cloudfront.net/media/images/1552025878271-DevOps--set2--1.jpg)\\n\\n--build a new image : \'newrepo\u2019 with tag:1.0 from the dockerFiles/dockerfile\\n\\ndocker build -t newrepo:1.1 dockerFiles/\\n\\n![dockerfile](https://d2o2utebsixu4k.cloudfront.net/media/images/1552025892215-DevOps--set2--2.jpg)\\n\\n-- Now create a container from the above image:\\n\\n![dockerfile](https://d2o2utebsixu4k.cloudfront.net/media/images/1552025914827-DevOps--set2--3.jpg)\\n\\n-- List all the containers\\n\\n![dockerfile](https://d2o2utebsixu4k.cloudfront.net/media/images/1552025930794-DevOps--set2--4.jpg)\\n\\n-- start the container\\n\\n![dockerfile](https://d2o2utebsixu4k.cloudfront.net/media/images/1552025951229-DevOps--set2--5.jpg)\\n\\n--List only the running containers\\n\\n![dockerfile](https://d2o2utebsixu4k.cloudfront.net/media/images/1552025971609-DevOps--set2--6.jpg)\\n\\n## 10. What is Puppet? What is the need for it?\\n\\nPuppet is a Configuration Management and deployment tool for administrative tasks.\\nThis tool helps in automating the provisioning, configuration, and management of Infrastructure and Systems.\\n\\n*   Puppet helps administrators to automate the process of manually creating and configuring Virtual machines.\\n*   Say, you have to bring up \'n\u2019 number of Servers with \'x\u2019 number of VMs(Virtual machines) on them. Each VM needs to be configured for certain users, groups, services, applications, and databases etc.\\n*   The entire infrastructure can be loaded up with the help of Puppet programs re-using the codes on multiple servers.\\n*   Key feature: Idempotency\\n*   The same set of configurations can be run multiple times to build a machine on a server, as puppet would allow only unique configurations to be run.\\n\\n## 11. What does \'Infrastructure as code\u2019 means in terms of Puppet?\\n\\nEntire Server Infrastructure setup configurations are written in terms of codes and re-used on all the Puppet Server agent\u2019s nodes(machines) that are connected via a Puppet master Server.\\nThis is achieved by the use of code snippets called \'manifests\u2019; that are configuration files for every Server agent node.\\n\\n*   Each manifest (program files with *.pp extension) consists of the resources and the codes.\\n*   We can review, deploy and test the environment configuration for development, testing and production environments.\\n*   Puppet manifests written once are deployed on any environment to build up the same infrastructure.\\n\\n## 12. What is Jenkins used for in DevOps?\\n\\nJenkins is a self-contained, open source automation server(tool) for continuous development.\\nJenkins aids and automates CI/CD process.\\n\\nIt gets the checked in code from VCS like Git using a \'git plugin\u2019, build the source code, run test cases in a production-like environment and make the code release ready using \'deploy\u2019 plugin.\\n\\n*   These continuous _delivery pipeline_s are written in a \'_Jenkinsfile\u2019_ which is also checked into project\u2019s source code and version controlled by Git.\\n*   Pipelines are a continuous set of jobs that are run for continuous delivery and these jobs are integrated at every section of the workflow.\\n*   Jenkins pipelines easily connect to Docker images and containers to run inside.\\n*   Pipelines easily provide the desired test environment without having to configure the various system tools and dependencies.\\n\\nSample Jenkins file\\n\\n```json\\npipeline {\\n   agent { docker { image \'ubuntu:latest\' } }\\n   stages {\\n       stage(\'build\') {\\n           steps {\\n               sh \'uname -a\'\\n           }\\n       }\\n   }\\n}\\n```\\n\\n*   Jenkins will start the container- ubuntu with the latest image and execute the test case steps inside it.\\n*   agent directive says where and how to execute the pipeline\\n*   jenkinsfile (declarative pipeline)\\n*   pipeline\\n*   Jenkins saves us the trouble of debugging after a huge commit history if there was a code break.\\n\\n## 13. How do you implement CI/CD using Jenkins?\\n\\n*   Continuous Integration using Jenkins and Git plugin\\n*   Create a new Jenkins item as a \'Pipeline\u2019.\\n*   Add \'Git\u2019 as branch source and give the Project repository url.\\n*   Every time source code is pushed to the remote git repository from the local git repo;\\n*   Jenkins job gets started(triggered).\\n*   Jenkins build the code and the output is available under \u201cconsole output\u201d\\n*   In the git repository add a \'jenkinsfile\u2019 commit and push the code to git repository.\\n*   Add a Jenkinsfile in the git repository\\n\\n```json\\npipeline {\\n   agent { docker { image \'ubuntu:latest\' } }\\n   stages {\\n       stage(\'build\') {\\n           steps {\\n               sh \'uname -a\'\\n           }\\n       }\\nstage(\'Test\') {\\n           steps {\\n               sh \'./jenkins/scripts/test.sh\'\\n           }\\n       }\\n   }\\n}\\n```\\n\\n## 14. Mention some post condition pipelines options that you used in Jenkinsfile?\\n\\nWe can mention some test conditions to run post the completion of stages in a pipeline.\\n\\nCode snippet\\n\\n```json\\npost {\\nalways {\\necho \u201cThis block runs always !!!\u201d\\n}\\nsuccess {\\necho \u201cThis block runs when the stages has a success status\u201d\\n}\\nunstable {\\necho \u201cThis block is run when the stages abort with an unstable status\u201d\\n}\\n}\\n```\\n\\nHere are the post conditions reserved for jenkinsfile:\\n\\n*   **always:**\\n\\nRun the steps in the post section regardless of the completion status of the Pipeline\u2019s or stage\u2019s run.\\n\\n*   **unstable:**\\n\\nOnly run the steps in post if the current Pipeline\u2019s or stage\u2019s run has an \\"unstable\\" status, usually caused by test failures, code violations, etc.\\n\\n*   **aborted:**\\n\\nOnly run the steps in post if the current Pipeline\u2019s or stage\u2019s run has an \u201caborted\u201d status.\\n\\n*   **success:**\\n\\nOnly run the steps in post if the current Pipeline\u2019s or stage\u2019s run has a \\"success\\" status.\\n\\n*   **failure:**\\n\\nOnly run the steps in post if the current Pipeline\u2019s or stage\u2019s run has a \\"failed\\" status.\\n\\n*   **changed:**\\n\\nOnly run the steps in post if the current Pipeline\u2019s or stage\u2019s run has a different completion status from its previous run.\\n\\n*   **cleanup:**\\n\\nRun the steps in this post condition after every other post condition has been evaluated, regardless of the Pipeline or stage\u2019s status."},{"id":"/2019/05/10/docker-vs-VMs","metadata":{"permalink":"/profile/blog/2019/05/10/docker-vs-VMs","source":"@site/blog/2019-05-10-docker-vs-VMs.md","title":"Docker Vs Virtual Machines(VMs)","description":"Virtualization comes in handy with the need of running multiple simulated environments in a single machine without installing and configuring each instance. Virtualization has its used cases in many scenarios such as testing your app in multiple environments/devices.","date":"2019-05-10T00:00:00.000Z","formattedDate":"May 10, 2019","tags":[],"readingTime":20.2,"hasTruncateMarker":true,"authors":[{"name":"Divya Bhushan","url":"https://github.com/divyabhushan","imageURL":"https://avatars0.githubusercontent.com/u/11659160?"}],"frontMatter":{"title":"Docker Vs Virtual Machines(VMs)","author":"Divya Bhushan","authorURL":"https://github.com/divyabhushan","authorImageURL":"https://avatars0.githubusercontent.com/u/11659160?"},"prevItem":{"title":"DevOps Interview Questions and Answers - Part I - Basics","permalink":"/profile/blog/2019/06/10/devops-interview-questions-answers"}},"content":"**Virtualization** comes in handy with the need of running *multiple simulated* environments in a single machine without installing and configuring each instance. Virtualization has its used cases in many scenarios such as testing your app in multiple environments/devices.\\n\\nBoth Dockers and Virtual Machines(VMs) are capable of providing virtualized runtime environments for your software/hardware needs.\\n\\nThis article is aimed at providing a basic understanding of the common and unique features each Docker and a VM possess.\\n\\n\x3c!--truncate--\x3e\\n\\nWhen you deal with multiple platforms with customized hardware/software specifications, you need to have an effective resource management in place. \\n\\n## What a machine needs?\\n\\nEach computing environment(machine) needs its component of _hardware resources_ and _software resources._\\n\\nAs more and more machines are needed, building up and administering many such stand-alone machines is not only cumbersome, time-consuming but also adds up to the cost and energy.\\n\\nApparently; to run a customized _High-power_ _Scalable_ _Server_ is a better idea to consolidate all the hardware and software requirements into one place and have a single server run and distribute the resources to many machines over a network.\\n\\nThat saves us time, resources, energy and revenue.\\n\\nThese gigantic servers are stored in a data warehouse called a _Datacenter__._\\n\\nBelow Diagram (2) indicates a single _server_ serving and sharing resources and data among multiple client machines\\n\\nDoes this look simplified enough? Yes of course!\\n\\nSo, this setup looks feasible we have a high-power, high-storage Server that gives resources to many smaller(resources) machines over a network.\\n\\n**How to manage huge data - Servers**\\n-------------------------------------\\n\\nWith _Internet Of Things_ in boom, Information is over-flowing with a huge amount of data; handling tremendous data needs more system resources which means more _Dedicated server_s are needed.\\n\\n### **Many Servers approach challenge:**\\n\\nRunning several _Dedicated servers_ for specific services such as Web service, application or database service as indicated in Diagram (3) is difficult to administer and consumes more energy, resources, manpower and is highly expensive.\\n\\nIn addition; resource utilization of servers is very poor resulting in resource wastage.\\n\\nThis is where simulating different environments and running them all on a single server is a smart choice; rather than having to run multiple physically distinct servers.\\n\\nThis is how Diagram (3) would change after consolidating different servers into one as shown in Diagram (4).\\n\\nSheet 2\\n\\n**Virtualization**\\n------------------\\n\\n### What is Virtualization\\n\\nThe above single server implementation can be defined as the following term.\\n\\n_Virtualization_ is a technique used to simulate and _pretend a single infrastructure_ resource (_hardware resources_ and _software resources_) _to be acting as many_ providing multiple functionalities or services without the need to physically build, install and configure.\\n\\nIn other words;\\n\\nRunning _multiple simulated environments in a single machine without installing and configuring them_ is called _Virtualization_.\\n\\nTechnically speaking;\\n\\nVirtualization is an abstract layer that shares the infrastructure resources among various simulated _virtual machines_ without the need to physically set up these environments.\\n\\nDiagram (5) displays different virtual Operating systems are running on the same machine and using the same hardware architecture of the underlying machine.\\n\\n### _What is a Virtual machine_\\n\\nThe simulated virtualized environments are called virtual machines or VM.\\n\\nVirtual machine is a _replication/simulation_ of an actual physical machine.\\n\\nA VM acts like a real physical machine and uses the physical resources of the underlying host OS.\\n\\nA VM is a running _instance of a real physical machine._\\n\\n### _Need_ _for_ _virtualization_\\n\\nSo; we have an overview of virtualization, let us examine when should we virtualize and what are the benefits of virtualization?\\n\\n1.  **Better resource management and** **cost-effective**: as indicated in Diagram (6) and Diagram (7); hardware resources are distributed wisely on need basis to different environments; all the virtual machines share the same resources and reduce resource wastage.\\n2.  **Ease of quick administration and maintenance**: It is easier to build, install, configure one server rather than multiple servers. Updating a patch on various machines from a single virtualized server is much more feasible.\\n3.  **Disaster recovery**: Since all the virtualized machines reside on the same server and are treated as mounted volumes of data files, it is easier to back up these machines. In case of a disaster failure (power failure, network down, cyber-attacks, failed test code, etc) VM screenshots are used to recover the running state of the machine and the whole setup can be built up within minutes.\\n4.  **Isolated and independent secure test environment**: virtualization provide an isolated independent virtual test environment to test the legacy code or a vendor-specific product or even a beta release or say a corrupt code without affecting the main hardware and software platform. (This is a contradictory statement though; will discuss more under types of virtualization)  \\n    These test environments like dev, uat, preprod, prod etc..can be easily tested and discarded.\\n5.  **Easily scalable and upgradable**: Building up more simulated environments means spinning up more virtual machines. Also upgrading VMs is as good as to run a patch in all VMs.\\n6.  **Portable**: Virtual machines are lightweight compared to the actual running physical machines; in addition, a VM that includes its own OS, drivers, and other installation files is portable on any machine. One can access the data virtually from any location.\\n\\nThe screenshot of activity monitor below compares the CPU load:\\n\\n_Implementation_ \\n-----------------\\n\\n### _a) What is hypervisor and its types__?_\\n\\nAs discussed in the previous section; virtualization is achieved by means of a virtualized layer on top of hardware or a software resource.\\n\\nThis abstract layer is called a _hypervisor._\\n\\nA hypervisor is a virtual machine monitor (VMM)\\n\\nThere are 2 types of hypervisors: Diagram (8)\\n\\n1.  Type-1 or bare-metal hypervisor\\n2.  Type-2 or hosted hypervisor\\n\\n_Type-1 or bare-metal_ hypervisor is installed directly on the system hardware, thus abstracting and sharing the hardware components with the VMs.\\n\\n_Type-2 or hosted hypervisor_ is installed on top of the system bootable OS called host OS; this hypervisor abstracts the system resources visible to the host OS and distributes it among the VMs.\\n\\nBoth have their own role to play in virtualization.\\n\\n### _b) Comparing hypervisor types_\\n\\nType-1 or bare-metal hypervisor\\n\\nType-2 or hosted hypervisor\\n\\nInstalled _directly on the infrastructure_\\\\-OS independent and more secure against software issues.\\n\\nInstalled _on top of the host OS_\\\\-more prone to software failures.\\n\\n_Better resource flexibility_: Have direct access to the hardware infrastructure (Hard-drive partition, RAM, embedded cards such as NIC). Provide more flexibility and scalability to the VMs and assign resources on a need basis.\\n\\n_Limited resource allocation_: Have access to just the resources exposed by the host OS.\\n\\nVMs installed will have limited access to hardware resources allocated and exposed by the host OS.\\n\\nSingle point of failure: A compromised VM may affect the kernel. Extra security layers needed.\\n\\nA compromised VM may affect only the host OS, kernel still remains unreachable.\\n\\nLow latency due to direct link to the infrastructure.\\n\\nHigh latency as all the VMs have to pass through the OS layer to access the system resources.\\n\\nGenerally used in Servers\\n\\nGenerally used on small client machines\\n\\nExpensive\\n\\nLess expensive\\n\\nType-1 Hypervisors in market:\\n\\nVMWare ESX/ESXi\\n\\nHyperkit (OSX)\\n\\nMicrosoft Hyper-V (Windows)  \\nKVM(Linux)\\n\\nOracle VM Server\\n\\nType-2 Hypervisors in market:\\n\\nOracle VM VirtualBox\\n\\nVMWare Workstation\\n\\nParallels desktop for MAC\\n\\n_Types of virtualization_\\n-------------------------\\n\\nBased on what resource is virtualized, there are different classifications of virtualization.\\n\\nServer, Storage device, operating system, network\\n\\n**Desktop** **virtualization**: Entire desktop environment is simulated and distributed to run on a single server all at once. A desktop virtualization allows administrators to manage, install, configure similar setups on many machines. Upgrading all the machines with a single patch update or security checks becomes easier and faster.\\n\\n**Server** **virtualization**: Many dedicated servers can be virtualized into a single server that provides multi-server functionality.\\n\\n**Example:**\\n\\nMany virtual machines can be built up sharing the same underlying system resources.\\n\\nStorage, RAM, disks, CPU\\n\\n**Operating system** **virtualization**: This happens at the kernel level Hypervisor on hardware type 2 bare-metal One machine: Can boot up as multiple OS like Windows or Linux side-by-side\\n\\n**Application virtualization**: Apps are packaged and stored in a virtual environment and are distributed across different VMs. Example Microsoft applications like excel, MS word, Powerpoint etc, Citrix applications.\\n\\n**Network functions virtualization**: Physical network components such as NIC cards, switches, routers, servers, hubs, and cables are all assembled in a single server and used virtually by multiple machines without having the load of installing them on every machine.\\n\\nVirtualization is one of the building blocks and driving force behind [_cloud computing_](https://www.ibm.com/cloud/learn/cloud-computing).\\n\\nCloud computing provide virtualized need-based services. This has given an uplift to the concept of virtualization.\\n\\nA quick mention of various cloud computing models/services are listed below:\\n\\n**SaaS** \u2013 Software as a Service\u2013 end-user applications are maintained and run by service providers and easily distributed and used by the end users without having to install them.\\n\\nTop SaaS providers: Microsoft (Office suite, CRM, SQL server databases), AWS, Adobe, Oracle (ERP, CRM, SCM), Cisco\u2019s [Webex](https://www.webex.com/de/index.html), [GitHub](https://github.com/) ( git hosting web service)\\n\\n**PaaS** \u2013 Platform as a Service \u2013 computing infrastructure(hardware/software) is maintained and updated by the service provider and the user just have to run the product over this platform.\\n\\nTop Paas providers: [AWS beanstalk](https://aws.amazon.com/elasticbeanstalk/), [Oracle Cloud Platform (OCP)](https://www.oracle.com/index.html), [Google App Engine](https://cloud.google.com/appengine/)\\n\\n**IaaS** \u2013 Infrastructure as a Service \u2013 Provide infrastructure such as servers, physical storage, networking, memory devices etc. Users can build their own platform with customized operating system and applications.\\n\\nKey IaaS providers: Amazon Web Services, [Microsoft Azure](https://azure.microsoft.com/en-us/), Google compute engine, Citrix\\n\\n**Conclusion:**\\n\\nWe now have a fair understanding of types of virtualization and how they are implemented.\\n\\n**Containerization**\\n--------------------\\n\\nThough virtualization has its pros; there are certain downsides of virtualization such as:\\n\\n*   Not all systems can be virtualized always.\\n*   A corrupt VM is sometimes contagious and may affect other VMs or the kernel in-case of a _Type-1 or bare-metal_ hypervisor.\\n*   Latency of virtual disks due to increased payload on the CPU resources with a higher number of VMs\\n*   Unstable performance\\n\\nAn alternative approach to overcome the above flaws of virtualization is to _Containerize_ the applications and the run-time environment together.\\n\\n_What is containerization_ \\n---------------------------\\n\\n_Containerization_ is an OS-level virtualization; wherein the entire build of an application along with run-time environment is encapsulated or bundled up in a package.\\n\\nThese packages are called _containers_.\\n\\nContainers are lightweight virtualized environments. These are independent of the infrastructure both hardware and software.\\n\\nThe run-time environment includes the operating system, binaries, libraries, configuration files and other applications as shown in Diagram (9).\\n\\n_What is a Docker_\\n-----------------\\n\\n_Dockers_ provide an excellent framework for containerization and allow to build, ship, and run distributed applications over multiple platforms.\\n\\nDocker framework is setup as a _docker engine_ installed on host OS and a [docker](https://www.knowledgehut.com/devops/docker-training) _daemon_ (background process) process is started that manage the virtual _containers_.\\n\\nRefer Diagram (10) that shows a Docker engine with 3 containers residing on host OS (MAC OS).\\n\\nAn instruction file called _dockerfile_ is written with a set of system commands that change the filesystem such as add, copy or delete commands, run commands, install utilities, system calls etc\u2026\\n\\nThis _dockerfile_ is built and packaged along with its run-time environment as an executable file called a _docker image._\\n\\nDocker daemon services run these images to create _docker containers._\\n\\n> _Docker container is a run-time instance of an image_\\n\\nIt is wise to say that many images (or layers of instruction files) make up a container.\\n\\nDocker containers have a compact packaging and each container is well isolated.\\n\\nWe can run, start, stop, attach, move or delete containers as these runs as services on the host OS.\\n\\nEach image is made up of different layers; each image based on top of the other with the customized command changes that we make.\\n\\nEvery time we make a change in the filesystem, each change related to the image is encapsulated in a new layer of filesystem and stacked up above the parent image.\\n\\nOnly the changed layers are rebuilt, rest of the unchanged image layers are reused.\\n\\nCertain docker commands ADD, RUN and COPY c_reate a new layer with increased byte size; rest of the commands simply adds up a new layer with zero-byte size._\\n\\n_These layers are re-used to build a new image, hence faster and lightweight._\\n\\nDocker images are also\\n\\nThe layer approach of an image every time there is a change in the image makes it possible to Version control the docker images.\\n\\nHere is a terminal recording that shows docker engine process and how images and containers are created.\\n\\nDocker documentation - to [create containers](https://docs.docker.com/get-started/part2/).\\n\\n### **Ppt diagram:**\\n\\nCode -> package -> build images -> registry hub -> download/pull image -> run container\\n\\nAnimation: sheet4\\n\\nLet\u2019s consider the docker container: divyabhushan/learn\\\\_docker hosted on docker hub.\\n\\nLatest tagged image: centOS\\\\_release1.2\\n\\n### What is the container environment?  \\n\\nBase OS: Centos:7\\n\\nUtilities: vim, yum, git\\n\\nApps/files: Dockerfile, myApp.sh, runtests.sh, data and other supporting files.\\n\\nGit source code: [dockerImages](https://github.com/divyabhushan/DockerImages_Ubuntu.git)\\n\\nDownload as: git clone [https://github.com/divyabhushan/DockerImages\\\\_Ubuntu.git](https://github.com/divyabhushan/DockerImages_Ubuntu.git)\\n\\nWhat does the container do?  \\nContainer launches \u201cmyApp.sh\u201d in Ubuntu:14.04 environment and run some scripts along with a set of post test\\\\_suites in the container (Ubuntu:14.04) and saves the output log file.\\n\\n_How to modify and build your own app_\\n--------------------------------------\\n\\n### **Step 1: pull** \\n\\n**1.1: Pull the docker image**\\n\\n  \\n\\n  \\n\\n**1.2: Run image to create a container and exit**\\n\\n  \\n\\n### **Step 2: modify**\\n\\n2.1: Start the container\\n\\n2.2: Attach to the container and make some changes\\n\\n### **Step 3: commit**\\n\\n3.1: Examine the history logs and changes in the container\\n\\n3.2: Commit the changes in container\\n\\n### **Step 4: push**\\n\\n4.1: Push new image to docker hub\\n\\nLet us see the steps in action:\\n\\n### **Step 1: pull** \\n\\ndocker image on your machine\\n\\n1.1: Pull the docker image\\n\\n**Command:**\\n\\ndocker pull divyabhushan/learn\\\\_docker:myApp\\\\_ubuntu\\\\_14.04\\n\\nView the image on system\\n\\ndocker images\\n\\nscreenshot\\n\\n**Command:**\\n\\ndocker run -it --name ubuntu14.04 0a6f949131a6\\n\\nRun command in ubuntu container and exit, the container is stopped on exiting out.\\n\\nView the stopped container with the \u2018ps -a\u2019 command.\\n\\n### **Step 2: modify**\\n\\nStart the container\\n\\n**Command:**\\n\\ndocker start <container\\\\_id>\\n\\nNow the container is listed as a running process Attach to the container and make some changes\\n\\n**Command:**\\n\\ndocker attach 7d0d0225778c\\n\\nedit the \u2018git configuration\u2019 file and \u2018myApp.sh\u2019 script\\n\\nContainer is modified and stopped\\n\\n### **Step 3: commit**\\n\\nExamine the history logs and changes in the container\\n\\nThe changes done inside the container filesystem can be viewed using the \u2018docker diff\u2019 command as:\\n\\n**Command:** \\n\\n```bash\\ndocker diff 7d0d0225778c\\n```\\n\\nCommit the changes in container\\n\\nDocker commit:\\n\\nUsage: docker commit \\\\[OPTIONS\\\\] CONTAINER \\\\[REPOSITORY\\\\[:TAG\\\\]\\\\]\\n\\ndocker commit -m \'new Ubuntu image\' 7d0d0225778c divyabhushan/learn\\\\_docker:ubuntu14.04\\\\_v2\\n\\nNew image is created and listed\\n\\n### **Step 4: push**\\n\\nPush new image to docker hub\\n\\n**Command:**\\n\\ndocker push divyabhushan/learn\\\\_docker:ubuntu14.04\\\\_v2\\n\\nPoint to note: just the latest commit change layer \u201850a5ce553bba\u2019 has been pushed, while the other layers were re-used.\\n\\nImage available on docker hub:\\n\\nThe latest tagged image can now be pulled from other machines; and run to create the same container environment.\\n\\n**Conclusion**: An image was pulled and run to create a container to replicate the environment. Container was modified, new changes were committed to form a new image. New Image pushed back on the docker hub and now available as a new tag ready to be pulled by other machines.\\n\\n**Difference between Dockers and Virtual machines**\\n---------------------------------------------------\\n\\n### _Tabular differences on various parameters_\\n\\n| Parameters                                                 | VMs                                                                                                                                                                                                                                                                   | Dockers                                                                                                                                                                                                                                                                                |\\n|------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| architecture                                               | Hardware-level virtualization. Each VM has its own copy of OS.                                                                                                                                                                                                        | Software level virtualization. Dockers have no own OS, run on host OS                                                                                                                                                                                                                  |\\n| Isolation                                                  | Fully isolated                                                                                                                                                                                                                                                        | Process or application-level isolation.                                                                                                                                                                                                                                                |\\n| Installation                                               | Hypervisor can run directly on the hardware resources or on the host OS.                                                                                                                                                                                              | Docker engine is installed on top of the host OS. A docker daemon process is initiated on the host OS. There is no separate OS for every container.                                                                                                                                    |\\n| CPU processing + performance                               | Slower: A VM contains the entire run-time environment that has to be loaded every time. Uses more CPU cycles; gives unstable performance.                                                                                                                             | Faster: Docker images are pre-built and share host resources as a result running an image as a container is lightweight and consumes less CPU cycle; gives a stable performance                                                                                                        |\\n| Hardware storage                                           | More storage space as each VM is an independent machine (OS). Example: 3 VMs of 800MB each will take 2.4 GB of space.                                                                                                                                                 | Docker containers are lightweight since do not require to load OS+drivers, run on host OS as processes.                                                                                                                                                                                |\\n| Portable                                                   | Dependency on host OS and hardware makes VM less portable. Importing a VM still requires manual setup such storage, RAM and network.                                                                                                                                  | Highly portable since lightweight and zero dependency on hardware.                                                                                                                                                                                                                     |\\n| Scalable and code-reusability                              | Spinning up more VMs still need administrative tasks such as distributing resources to VM. Running a new machine puts extra load on the system resources also re-managing earlier VMs becomes a task. Every VM keeps its own copy of resources-poor code-reusability. | Spinning up new docker containers simply means running pre-built images into containers as a process inside host OS. Containers are also configured on-the-fly passing parameters and run-time. Single image can be run and used to create many containers; encourage code-reusability |\\n| Resource utilization                                       | Static allocation results in resource wastage in case of idle VMs or if a VM\u2019s resource requirement increases.                                                                                                                                                        | Resources are dynamically allocated and de-allocated on the need basis by the docker engine.                                                                                                                                                                                           |\\n| Docker system prune or garbage collection                  | Virtual machines do not have an in-built prune mechanism, these have to be administered manually.                                                                                                                                                                     | Docker image and containers can be pruned; which frees up a sensible amount of storage and memory space and CPU cycles.                                                                                                                                                                |\\n| New environment                                            | Creating new VM from the scratch is a tedious, repetitive tasks. It involves installing a new OS, loading kernel drivers and other tools and configurations.                                                                                                          | Package the code and dependency files, build into an image, run the image to create a new container. Use an existing or a base image (dockerhub- scratch) to run and create more containers on the go.                                                                                 |\\n| Web-hosted Hub                                             | No web hosted hub for VMs                                                                                                                                                                                                                                             | dockerHub provides an open-source reliable trusted source of pre-built images that can be downloaded to run new containers.                                                                                                                                                            |\\n| Version control (backup, restore,track history)(refer git) | Snapshot of VMs are not very user-friendly and consume more space.                                                                                                                                                                                                    | Docker images are version controlled.\xa0Every delta difference in each docker container can easily be viewed (demo: docker diff ).\xa0Any change in the image is stored as a different layered version. A reference link to older images saves build time and space.                        |\\n| Auto-build                                                 | Automation of creating VMs is not very feasible.                                                                                                                                                                                                                      | Docker images can also be auto-built from every source code check-in to GitHub (Automated builds on Dockerhub)                                                                                                                                                                         |\\n| Disaster recovery                                          | Tedious to recover from VM backup files.                                                                                                                                                                                                                              | Easier to restore docker images (like files) just like git source files in case images are version controlled. Backup images only have to be run to create containers. (refer: screenshot).                                                                                            |\\n| Update                                                     | All the VMs have to updated with the release patch.                                                                                                                                                                                                                   | A single image is updated, re-built and distributed across multiple platforms.                                                                                                                                                                                                         |\\n| Memory usage+speed                                         | Slower: Entire snapshot of a machine and the OS is loaded into the cache memory.                                                                                                                                                                                      | Real-time and fast: pre-built images. Only the instance, i.e, a container has to be run as a process and uses memory like an executable                                                                                                                                                |\\n| Data integrity                                             | VM behavior may change if the dependency includes beyond the VM boundaries. (example: an app depends on production host network settings)                                                                                                                             | Same behavior of apps in any environment                                                                                                                                                                                                                                               |\\n| security                                                   | More secure: A failure inside a VM may reach its guest OS but not the host OS or other virtual machines. Type-2 hypervisor though has a risk of kernel attack.                                                                                                        | Less secure: If a docker container compromised; underlying OS and hence all the containers may be affected since they share the same host kernel. OS Kernel may also be risked.                                                                                                        |\\n| Key providers                                              | Red hat KVM, VMWare, Oracle VM VirtualBox, Mircrosoft Hyper-V, Citrix XenServer                                                                                                                                                                                       | Dockers,\xa0Google kubernetes Engine,\xa0AWS Elastic Container service                                                                                                                                                                                                                       |\\n| Data authentication                                        | Lot of software licenses.                                                                                                                                                                                                                                             | Docker maintains inbuilt content trust to verify published images.                                                                                                                                                                                                                     |\\n\\n _When to use VM or a Docker_\\n-----------------------------\\n\\nWhen the need is an isolated OS, go for VMs.\\n\\nFor a hardware and software independent isolated application that needs fast distribution on multiple environments, use dockers.\\n\\n*   ### Docker use-case:\\n    \\n\\nExample: A database application along with its database\\n\\nConsider the docker image - [Oracle WebLogic Server](https://hub.docker.com/_/oracle-weblogic-server-12c?tab=description) on [Docker Hub](https://hub.docker.com/).\\n\\nThis image is pre-built Oracle WebLogic Server runtime environment, including Oracle Linux 7 and Oracle JDK 8 for deploying Java EE applications.\\n\\nTo create Server configurations on any machine, just download this image and run to create and start a container.\\n\\nThere is no need to install and configure JDK, Linux or other run-time environment.\\n\\n*   ### Do not use Docker use-case:\\n    \\n\\nThe application depends on utility outside the docker container.\\n\\nCode developed on dev machine with base OS as MAC; needs certain firewall setting on say Ubuntu OS.\\n\\nHow can the code be tested on the production ubuntu OS firewall while running from MAC OS docker container?\\n\\nSolution:  Install a virtualization software on host OS-MAC; Create a VM (Virtual machine) with host OS as Ubuntu (same as production environment).\\n\\nConfigure the desired firewall settings on host VM \u2013 Ubuntu; import the test code inside Ubuntu and test.\\n\\n*   ### Use a VM:\\n    \\n\\nFor Embedded systems programming, a VM is installed that connects to the system device drivers, controllers and kernel.\\n\\n*   ### Virtualization used along with docker:\\n    \\n\\nAn extension to the previous scenario would be if you would want to also test your python application in the host OS-Ubuntu VM without having to set up the python exe and its libraries and binaries.\\n\\nAll you have to do is: Install [Docker](https://www.knowledgehut.com/devops/docker-training) engine for Ubuntu OS and pull the python image from Docker hub as:\\n\\ndocker pull python:tag \\\\[ tag is the python version-choose the appropriate version \\\\]\\n\\ndocker pull python:2.7\\n\\nRefer: [Python image](https://hub.docker.com/_/python/)\\n\\nEither write a Dockerfile to import/copy entire source code to python environment or directly run the image passing the script path as below:\\n\\nCommand:\\n\\n$docker run -it --name my-python-script -v \u201c$PWD\u201d:/usr/src/myapp -w /usr/src/myapp python:2.7 python my-application.py\\n\\nCommand options:\\n\\n\\\\-v = volume list-bind mount a volume \\\\[mount present working directory onto /usr/src/myapp inside container\\\\]\\n\\n\\\\-w = workdir string-working directory inside the container\\n\\nMoreover; you can also test your python code in more than one version by downloading different python images, running them to create different containers and running your app in each container.\\n\\nWhat\u2019s exciting here is that once the code tested in each python environment; you could quickly work on the test results and drop the containers. And deploy the code to production only once code tested against various python versions.\\n\\n#### _Final thoughts_  \\n\\nVMs and dockers are compatible with each other. Dockers are not here to replace Virtual machines.\\n\\nBoth serve the same purpose of virtualizing the computing and infrastructure resources for optimized utilization.\\n\\nUsing both Virtual machines and dockers together can yield better results in virtualization.\\n\\nWhen one desires a fast, lightweight, portable and highly scalable hardware-independent environment for multiple applications isolation; wherein security is not the major concern; Dockers is the best choice.\\n\\nUse a VM for embedded systems that are integrated with hardware; such as device driver or kernel coding.\\n\\nA scenario simulating an infrastructure setup with a high resource control and dependency on system resources; VMs are a better choice.\\n\\nUse of Dockers inside VM\\n\\nCI/CD pipelines scenario:\\n\\nVirtualization enables a smooth CI/CD process flow by promoting the users to concentrate only on developing the code on a working system that is set up for automated continuous integration and deployment without having to duplicate the entire setup each time.\\n\\nA virtualized environment is set up; either using a VM or a docker image that takes care of the automatic code check-ins, builds, regression testing, and deployments on the server.\\n\\n> Originally posted on [KnowledgeHut](https://www.knowledgehut.com/blog/devops/docker-vs-vm)"}]}')}}]);